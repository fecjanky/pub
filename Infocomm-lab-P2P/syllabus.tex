\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage[hidelinks]{hyperref}

\usepackage{caption}

\usepackage{listings}

\usepackage{color}

% *** GRAPHICS RELATED PACKAGES ***
%\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
%\usepackage[dvips]{graphicx}
% to place figures on a fixed position
\usepackage{float}

\usepackage[margin=1in]{geometry}

\title{P2P – syllabus}
\author{}
\date{}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Compared to traditional client-server operation peer-to-peer solutions (P2P) offer a more direct and dynamic
solution. They provide searching services in a global network and allow direct, point to point data transfers
bypassing the administrative and performance bottlenecks of servers.
The well-know and most used P2P applications -- like file exchange, VoIP and on-line gaming -- affects many users and
generate the significant portion of today's Internet traffic. Covering all aspects of P2P network during this lab would
be impossible therefore only some key parts will be demonstrated. Since file transfer is currently to most utilized
application we are going to cover that during this lab.

\section{P2P file exchange}

The first system specifically designed for file exchange called \emph{Napster} was released in 1999. The file exchange
networks were growing rapidly after that, several architectures had been developed and had been categorized  into 3
main categories. This chapter gives an overview on each and then discusses BitTorrent in detail which is going to be
the main focus during this lab.

\subsubsection{Client-server systems}

The architecture of Napster is considerably simple. The basis of it is one central server and each user is connected to
this server with its client program. Downloading is controlled by the central server however the data transfer is
between the two requesting and the offering clients directly without going through the server. This property makes it
eligible Napster to be categorized as a P2P network due to utilizing the storage capacity and network access bandwidth
of all of its clients. An obvious shortcoming of Napster's architecture is the reference to the central server park --
shutdown of that would bring down the whole network. This had happened several times in the past due to ongoing legal
procedures between RIAA (American copyright protection entity) and Napster because of the contents that were available
on the network.

The focus transitioned on changing the principal design of the architecture and was looking for a solution that doesn't
rely on central entities i.e. it's fully distributed. Others were just trying to address security related problems. The
resulting architecture had the clients connect to a server but the server itself did not store an information about the
clients and many smaller servers were in operation instead of just one central entity. The most notable implementation
of this architecture is DirectConnect.

\subsubsection{Distributed systems}

After the fall of Napster it had been realized that the main vulnerability of these systems is the presence of the
central server. Later on as a result of repeated strikes against DirectConnect based networks most of the file exchange
users started looking for a system which operates in a fully distributed way i.e. it doesn't have any central entity.
The distributed architecture had its own cons compared to the client-server based approach. In distributed systems the
number of network hops could be arbitrarily large resulting in greater network traffic in the network itself and in
increased execution time of downloads, queries, etc. Another theoretical problem is that connecting to a distributed
network is not trivial -- due the missing central server -- the client who wants to connect has to obtain the network
address of some already connected client. Furthermore network administration had become virtually unmanageable.

Around the year 2000 several networks had been operated based on the principles described above. Almost every
programmer was developing P2P file exchange software in his free time. A variety of different but short-lived solutions
had been introduced in this era. However some of the systems have survived and gain popularity for example Gnutella
which is still one of the most popular network in some countries.

Gnutella newtork is truly peer-to-peer which means that its only contains clients communicating with each other; it
doesn't utilize any central or globally reachable component. As mentioned before there is a problem arising from new
client connections: it's necessary to know at least one already connected client. Initially there was no well
established methodology for that resulting in spreading the word about some clients that were already connected between
people to be able to connect to the network. There had been some search algorithms introduced to overcome this however
these did not operate effectively in global scale.

After a client got hold of a Gnutella member it could initiate a connection to it with a greeting message. The peer
receiving this message replies and indicates its availability. Furthermore it forwards the message to all of its known
peers - naturally these messages utilized a Time-To-Live (TTL) mechanism. This process repeats until the TTL reaches 0.
During the message's lifetime it gets process by a lot of network members -- technically all peers in 6 hops distance
from the edge peer. Afterwards the connecting peer initiates a connection to all reachable peers as a result then it is
tightly coupled to the network.

Gnutella provides almost complete security against legal concerns because the complete network is technically
non-discoverable. This non-discoverable property is one of the greatest drawbacks of it because due to the client's
relatively small horizon it it impossible to know many peers are using the network without being able to monitor the
whole Internet. Still there are only approximations about the total number of users which says it's around the
magnitude of 1000 connection per 1 peer.

The search messages are processed similarly to the connection initiation messages. The client floods the search message
to all of its connected peers. Those process the query and if any has a positive match it sends back a reply. The
message is then flooded onwards -- TTL mechanism still applies. In this way the search request	traverses all the way
through the client's horizon and the originating peer receives all the positive matches. The reply messages may contain
new peers that can be used for establishing new connections if the originating peer would like to broaden its horizon.

\subsubsection{Hierarchical systems}

The hierarchical systems combine the advantages of the previously described client-server and pure P2P networks.
It doesn't have central entities with global availability however not all peers are equal: some of the entities has
special responsibilities.
This type of network can be imagined as the special purpose entities -- a.k.a. super-clients -- are connected to each
other in a P2P fashion while regular clients are connected to these super-clients in a client-server fashion.
Kazaa (FastTrack) operates based on this principle.

Fastrack is basically based on later versions of Gnutella where only the \emph{Ultrapeers} are eligible to be connected
to the Gnutella network itself while the leaf clients can only connect to Ultrapeers. The Ultrapeers responsiblity
beside processing their own search operations is to serve the connected regular clients and take over their traditional
P2P tasks. In the Kazaa network the Ultrapeers are called \emph{Superpeers}.

When the user starts the client program that uses a proprietary algorithm to decide the role in which it will operate.
If it is going to be a Supernode it is trying to establish connections with other Supernodes. Due to the encrypted
communication it is unknown how Supernodes find each other but it's known that the client software contains a
hard-coded list of Supernodes that gets updated with each release. The Supernode under connection tries to connect to
Supernodes chosen from this list until it successfully connects to one of them. After successful connection the other
Supernodes will send the current list of Supernodes in the network so the newly connected Supernode can update its
database. It is unknown that how the list of Supernodes is created but it's likely that it is similar to the process
used by Gnutella. The protocol used between Supernodes are not known in details due the encrypted nature of the
communication but measurements showed that a Supernode connects to 25 other Supernodes on average.

FastTrack architecture is a fine fusion between the client-server and the pure P2P networks. The Supernodes backbone
that replaces the central server entity provides a scalability and high-availability. Flooding only takes part between
Supernodes as a consequence as more ordinal peers connect to the network the network traffic is not growing
proportionally -- it only increases between the high bandwidth Supernodes. This architecture resembles the architecture
of Skype -- not accidentally as the same developer team was responsible for developing both systems.

\section{BitTorrent}

The original purpose of BitTorrent released in 2001 was to decrease the load on servers. The problem to solve is that
given a server there are too many clients trying to download the same file. The sever slows as more and more clients
connects to it and in worst case it could crash. The key idea behind BitTorrent is that the clients could download
fragments of data from other clients as soon as the have it. The files are fragmented into smaller chunks that are
eligible to be download in any order, and not only from the server but from other clients that has the corresponding
fragments as well. As more and more clients download a fragment/piece from each other it will have greater availability
reducing the server load proportionally. The implementation uses two central functionality: the \emph{Seeder} is a
server that originally provided full access to the original file; the \emph{Tracker} is also a server that track the
downloading process so it can provide information on fragment availability -- which fragment can be downloaded from
which clients -- for the newly connected clients.

In terms of network size this concept is a refinement of the transition between Napster and DirectConnect. While
Napster aimed for a globally uniform network DirectConnect focused on creating smaller, topic based networks. These
kind of smaller networks received connections from clients who were interested in the content of the specific topic
provided by the network as a result reducing the number of ``unnecessary" files shared on a single network. BitTorrent
continues along this basic concept to an extreme where micro networks are established for sharing a single file.
Naturally one client can be a part of several of these networks still it only has to be a member of those in which it
has interest in. This hypothesis is backed by measurements -- in a file sharing system the 90\% of the traffic is
generated by the 5-10\% of the files shared.

Due to this property the system offers extremely fast download speeds where most of the time the bottleneck is the
downlink bandwidth of the client's access network. Despite its differences compared to the other file sharing solutions
the potential of BitTorrent has been realized quickly. There were minor modifications in the implementation but thanks
to the open source nature of the project new releases were published repeatedly that offered better and better file
sharing capabilities from release to release. To be specific the main function of it was file downloading since it
didn't have any other features.

For practical reasons due to file sharing the following modifications had been done: there are special purpose web
sites from where the users can download torrent files. These sites can vary but the same torrent file can be found on
multiple sites. Based on the torrent files the client can find the appropriate Tracker and Seeder servers.

Another speciality is the Seeder server doesn't have a distinguished role. In this variant a client that has finished
downloading -- i.e. it has all the fragments of a file -- it becomes a Seeder itself. As time progresses the number of
Seeders will increase in the network further increasing the download speed. Despite the Seeder server the terminology
calls Seeders those clients as well which are sharing the complete files for the other clients.

In order to connect to the network the user downloads the torrent file and the BitTorrent client can use that to locate
the file's Tracker and Seeder servers. Using the Tracker the client can find other clients that has only downloaded
fragments of the current file so they are also eligible to download from. In newer versions the torrent file can refer
to multiple Tracker and Seeder servers as a result providing higher availability in case of a server outage.

The content sharing is done slightly differently compared to other P2P file sharing networks. BitTorrent had been
developed for sharing specific files and content not for sharing more and more files as a consequence it does not
support sharing full folders.

To be able to share first one has to select one (or more) Tracker server. After the Tracker has been selected a torrent
file has to be created that refers to the creator host as Seeder and to the chosen Tracker server as Tracker.
Furthermore the candidate file to be shared has to be fragmented into pieces with corresponding hash information
calculated and written into the torrent file. Finally the torrent file has to be published for example on some of the
well-known sites in order to be found by those whom are interested in the content. Some of the web-sites provide
searching services if some of the meta-data are also given.

As soon as the clients obtain fragments of the file they are sharing it wit the other by notifying the Tracker server
that they also have that particular segment. After the full file has been downloaded the client becomes a Seeder and it
can appear as a potential seeder in the torrent file.

Nonetheless the client programs are open source most of them prevent ``free-riding" i.e. the download bandwidth should
be proportional to the upload bandwidth in order to restrict leeching on the other clients.

Since in case of BitTorrent there are so-called micro-network that are specifically established to broadcast a given
file there are no means to execute queries for specific files. Search operations usually provided by those web-sites
that are specialized in storing torrent files. On these servers one can save auxiliary meta-data for any torrent file
so it can be searched through the web UI. In practice a search results in a torrent file that leads the client to the
corresponding file-sharing micro-network.

Furthermore the client can connect to a Tracker server using the info obtained from the torrent file. The Tracker then
directs the client to those other clients and Seeders from where fragments of the given file can be downloaded. After a
piece has been downloaded the Tracker records it as a result subsequent clients can download the same fragment from the
previous client.

Actually the fragments of a file are further divided into smaller pieces so even in case of short availability shared
full downloads can be successful. If a client successfully obtain a sub-fragment of a fragment the software will give
precedence to download other sub-fragments from the same fragment. Given this algorithm it's more likely to obtain full
segments that are eligible to be downloaded by others. The integrity of a fragment is verified by checking the
calculated has value of the downloaded fragment with the one present in the torrent file. Another optimization is that
the Tracker counts that which fragment has the lowest availability in the network and it directs a newly connected
client to download those fragments. This results in off-loading the server and on the other hand it also provides high
availability of the file in the network so in case of the Seeder is lost the rare segments could be downloaded as well.
Nevertheless clients benefit from downloading fragments with low availability since as soon as it obtains such a piece
it will be selected as an uploader with higher probability. This ensures more downloader and provides greater global
download bandwidth as well as greater download bandwidth for itself. To summarize the file downloading works like the
client connect to as many clients -- that have full fragments available -- as possible and as soon as the downloading
client obtains a full fragment it shares it and provides availability for these fragments.

The time that takes to download files varies greatly it's almost unpredictable. Slow start of download characterizes
the system that is followed by exponential speed-up. In the beginning only the Seeder has the file as a result all
clients will download from that. As time passes more and more peers will have fragments available thus off-loading the
server gradually and providing more and more upload bandwidth for the given file.

Torrent file use \emph{bencode} for encoding data. This is a string based encoding that can be decoded easily. This
encoding supports two primitive types and one complex type. The primitive types can be used for storing strings and
integers respectively and the complex types can be used for building lists and dictionaries. Recursive composition of
complex types is supported -- e.g. a list can contain another list as an element.

The integers are represented by writing the number to be encoded between  a leading ``i" and a trailing ``e" -- e.g
`13' is encoded as `i13e'.

Strings are encoded in way that the length of the string is written before a colon that is followed by the actual
string. For example the string ``apple" in encoded format is `5:apple'. Important to notice that there is no terminator
that follows the string, the length of the string can be determined from the leading length indicator.

List can contain arbitrarily many entries that are encoded between a leading ``l" and a trailing ``e" character. For
example ``l5:applei13ee" encodes a list of two where the first element is the string ``apple" that is followed by the
number `13'.

Dictionaries -- similarly to lists -- are encoded between leading ``d" and a trailing ``e" character however the number
of elements must be an integral multiple of 2 where the fist element of a pair is going to be the key and the second
will be the value. For example the ``d5:applei13e4:pear4:null" encodes a dictionary that assigns the number `13' to the
string key ``apple" and has value string ``null" assigned to the key ``pear".

\section{Topology-aware BitTorrent clients}

A peer-to-peer alkalmazások terjedésével, és a ma ugyan már csökkenő, de még mindig jelentős szerepükkel, nagy részt
hasítanak ki az Internet teljes forgalmából: egyes becslések szerint a peer-to-peer alkalmazások a teljes forgalomnak
akár 70\%-át is generálhatták egy időben. A gondot az okozza, hogy az overlay koncepció figyelmen kívül hagyj a
hálózati topológiát. A jelenleg legnépszerűbb BitTorrent alkalmazás nagyban növeli az ISP-k internetes linkekre
fizetett költségét az ISP-közi megnövekedett forgalom miatt. Ez az ISP-ket arra kényszeríti, hogy a felhasználók
peer-to-peer forgalmát korlátozzák, aminek sajnos elégedetlen felhasználókban nyilvánul meg.

A topológia-tudatos peer-to-peer alkalmazások ezen problémára próbálnak megoldást adni.

Az Ono, egy plugin a Vuze BitTorrent klienshez, célja az, hogy a (topológiai szempontból) véletlenszerű
peer-kiválasztási stratégia helyett a hálózatban közeli peer-eket részesítse előnyben. A közeli peer-ek egyrészt
várhatóan kisebb válaszidővel és nagyobb sávszélességgel rendelkeznek, másrészt a peer-ek ilyen módon történő
kiválasztása csökkenti az ISP-k közti forgalmat, gerinchálózati terhelést is. Az Ono plugin a peer-ek közelségének
meghatározásához a tartalomszolgáltató hálózatokon (CDN, pl. az Akamai hálózat) végzett méréseit használja.

A TopBT kliens aktívan fedezi fel a peer-ek hálózati közelségét és mind azt, mind az átviteli sebességet figyelembe
veszi, hogy magas letöltési sebességet nyújtson, miközben csökkenti a BitTorrent forgalom átviteli távolságát, és így a
teljes Internet forgalmát. A ping és a traceroute hálózati segédprogramokat használja a többi peer közelségének
felderítésére. Ellentétben az Ono pluginnel, a TopBT kliensnek a hatékony működéshez nincs szüksége az ISP-ktől, vagy a
CDN-ektől származó információkra, és nem igényli, hogy a többi peer szintén a TopBT klienst használja.

\section{P2P network using Mininet, OpenFlow switches and POX controller}
A Mininet, OpenFlow és POX kontroller bemutatása az OpenFlow \& Mininet mérés (MSc) Mérési Segédletben olvasható. A
továbbiakban a P2P mérésen használt Mininetes hálózatot mutatjuk be.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/halozat.png}
    \caption{Network architecture used in lab exercises}
    \label{fig:Lab-topo}
\end{figure}

A hálózatban 4 OpenFlow switch fut, ezekböl 3 (R1, R2, R3) router müködést szimulál static routeokkal. Kiemelendö a
szimuláció, ugyanis az OpenFlow switchek nem képesek ARP kérésekre válaszolni, így azokat vagy egy megfelelö kontroller
alkalmazásból kell kezelni, vagy esetünkben static MAC címeket használunk. A szimulált hálózatban alapszabály, hogy a
hostok a saját routereiket mint gatewayt mindig a csupa nullás (egyébként invalid) MAC címmel érhetik el. A hostok MAC
címjei pedig "00:00:<ROUTER-ID>:00:00:<ROUTER-LINK-ID>" séma alapján kerül kiosztásra. Ennek megfelelöen az R2-es
routerre kötött elsö host (leecher 2) a "00:00:02:00:00:01" MAC címmel rendelkezik.

Az R2-es routeren a következö OpenFlow statikus szabályok kerülnek beillesztésre:

\begin{lstlisting}[language=bash,frame=single,breaklines,caption={R2 router flow entry configuration},label=lst:R2-flow-config]
ovs-ofctl add-flow r2 'ip,nw_dst=89.97.0.0/16,action=mod_dl_src:00:00:00:00:00:00,output:4'
ovs-ofctl add-flow r2 'ip,nw_dst=201.42.54.65/28,action=mod_dl_src:00:00:00:00:00:00,output:5'
ovs-ofctl add-flow r2 'ip,nw_dst=23.53.32.53/32,action=mod_dl_dst:00:00:02:00:00:01,output:1'
ovs-ofctl add-flow r2 'ip,nw_dst=23.99.30.4/32,action=mod_dl_dst:00:00:02:00:00:02,output:2'
ovs-ofctl add-flow r2 'ip,nw_dst=23.0.3.78/32,action=mod_dl_dst:00:00:02:00:00:03,output:3'
ovs-ofctl add-flow r2 'arp,action=output:1,2,3' 
\end{lstlisting}

Az elsö két szabály a két szomszédos router IP cím tartományát fedi le. Látható, hogy a szomszédos routerek tartományai
/16 és /28-as netmaskkal rendelkeznek. Ha ezekben az IP tartományokban érkezik egy csomag, akkor történik egy forrás
MAC állítás (cél MAC állításra is szükség lenne a pontos router müködés szimulálására, de esetünkben ez kihagyható,
mivel a túloldalon is általunk programozott OpenFlow eszköz van - egyszerübb flow bejegyzések) és kiküldés a megfelelö
porton. Hostok esetében a match pontosan akkor törénik, ha az IP címnek mind a 32 bitje felveszi a célcímben megadott
IP címet. Egyezés esetén a cél MAC a host MAC címe lesz, és kiküldi a switch/router a megfelelö porton. Az utolsó sor
azt a könnyítést adja, hogy ugyanazon a subneten levö hostoknak (azaz ugyanarra a switchre/routerre bekötött) ne
kelljen statikusan megadni a MAC címjeit. Mivel az ARP egy broadcast címre épülö protokoll, ezért floodolni kell,
ugyanakkor figyelemben kell tartani, hogy nem minden portra kell floodolni. A router szétbontja a hálózatokat broadcast
domainekre, így a router szimulációjánál is figyelembe kell venni ezt a tulajdonságot. Ha az OpenFlow switchnek flood
paramétert adnánk meg, akkor a többi routert szimuláló switchnek is elküldenénk, amik szintúgy továbbítanák minden
portjukon, azaz egyetlen ARP csomag képes olyan broadcast stormot generálni, ami megbénítja a Mininet szimulációt
futtató gazdagépet.

A 4. switch (Firewall) nem rendelkezik statikus OF bejegyzésekkel, tisztán a kontrollertöl függ. A mérési feladatok
során a kontroller programozásával kell szürésí intelligenciát vinni a tüzfalba, hogy a torrent forgalmat megfelelöen
szürje, vagy éppen shape-elje. A tüzfal két porttal rendelkezik, és alapbeállításban a kontroller egy repeater funkciót
valósít meg a switchen, azaz ami egyik porton beérkezik, az a másikon kimegy. A tüzfal a kontroller fele Packet-In
üzenetekben küldi az egyik porton beérkezett csomagot, és ennek megfelelöen a POX kontroller alkalmazás a
handlePacketIn metódusában kezeli le ezeket a csomagokat. Az intelligenciát is ebben a metódusban kell implementálni. A
beérkezett csomag parsolását a következö függvényhívás végzi:
\begin{lstlisting}[language=python,frame=single,breaklines]
packet = event.parsed  
\end{lstlisting}

Ez után a 'packet' objektumtól már egyszerüen lekérdezhetöek az OpenFlow által ismert paraméterek. Például a beérkezö
port sorszáma a "packet.port", míg egy specifikus TCP port megtalálása és szürése a következöképpen nézhet ki:

\begin{lstlisting}[language=python,frame=single,breaklines]
tcpp = event.parsed.find( 'tcp' )

if not tcpp: return # Not TCP

if tcpp.srcport in block_ports or tcpp.dstport in block_ports:

# Halt the event, stopping l2_learning from seeing it

# (and installing a table entry for it)

core.getLogger( "blocker" ).debug( "Blocked TCP %s <-> %s" ,

tcpp.srcport, tcpp.dstport)

event.halt = True
\end{lstlisting}

Mivel az OpenFlow nem néz L4-nél magasabb rétegekbe, ezért például egy BitTorrent csomag header mezöinek az elemzése
problémásabb, ilyenkor muszáj az L4-es payload elemzése a "payload" property segítségével.

\appendix

\section{Entry quiz sample questions}

\section{Input files for the lab excercise}
\begin{itemize}

    \item

          \href{https://qosip.tmit.bme.hu/foswiki/pub/Meres/P2PAlkalmazasokMeresiSegedlet/p2pMeresTopo.py.txt}{p2pMeresTopo.py.tx
              t}: Mininet topologia P2P mereshez, futtatas: 'sudo python p2pMeresTopo.py'

    \item

          \href{https://qosip.tmit.bme.hu/foswiki/pub/Meres/P2PAlkalmazasokMeresiSegedlet/p2pMeresTopo.py.txt}{p2p.py.txt}: POX
          kontroller alkalmazas, bemasolando pox/ext konyvtarba, futtatas: './pox.py p2p'
\end{itemize}

\section{Lab exercises}

\end{document}